---
title: "Practice Activities"
author: "William Hopper"
date: "`r format.Date(Sys.Date(), '%m/%d/%Y')`"
output:
  html_fragment
editor_options: 
  chunk_output_type: console
---

<style>p, code, li {font-size: 20px; line-height: 1.5em;}</style>

```{r, include=FALSE}
library(moderndive)
library(ggplot2)
library(infer)
library(dplyr)
library(purrr)
library(tidyr)
```


## Data set links

- `ncbirths`: https://wjhopper.github.io/SDS-201/data/ncbirths.csv
- `ncbirths_population`: https://wjhopper.github.io/SDS-201/data/ncbirths_population.csv

## Chapter 6 Problems
1. For the `ncbirths` data set, consider the bivariate regression model `weight ~ smoker` that uses smoking status to explain variability in infant birth weights. Visualize the relationship between these variables, and compute the $R^2$ value for the regression model. Based on the visualization and the $R^2$ value, do you think think the bivariate `weight ~ smoker` model is a more useful model than the empty model `weight ~ NULL`?

2. For the `ncbirths` data set, consider the following two regression models that use smoking status and mother's age to explain variability in infant birth weights:

    1. The parallel slopes (a.k.a. additive) model: `weight ~ smoker + mage`
    2. The interaction model: `weight ~ smoker * mage`
    
    Visualize the relationship between these variables (including the regression lines for both models; remember that you can use the `geom_parallel_slopes` function from the `moderndive` package) and compute the *adjusted* $R^2$ value for both regression models.  Based on the visualization and the adjusted $R^2$ value, which model do you prefer? Put another way, does the parallel slopes or the interaction model provide the most accurate *and* concise description of the data?
    
## Chapter 7 Problems

Using the `ncbirths` and `ncbirths_population` data sets, answer the following questions:

1. What are the exact mean and variance of the sampling distribution of the average age of mothers giving birth in North Carolina in 2004 (assuming 125 mothers were observed in each sample)?
2. What are the estimated mean and variance of the sampling distribution of the average age of mothers giving birth in North Carolina in 2004, based on the first 125 observed values in the `ncbirths` sample? What about based on the second set of 125 observed values?
3. Which value from question 2 do you find more surprising?
4. What information can we learn by examining the sampling distribution of the sample mean?

## Chapter 8 Problems

Consider the following plot, which shows 100 confidence intervals around 100 different sample proportions, created using the percentiles of the bootstrap distribution. The red vertical line is the true population proportion, light grey horizontal lines represent confidence intervals whose endpoints contain the true population proportion, and black horizontal horizontal lines represent confidence intervals whose endpoints **do not** contain the true population proportion.

```{r, echo=FALSE, fig.width=7, fig.height=10, fig.align='center', cache=TRUE}
p_red <- bowl %>% 
  summarize(prop_red = mean(color == "red")) %>% 
  pull(prop_red)

set.seed(5)

# Function to run infer pipeline
bootstrap_pipeline <- function(sample_data){
  sample_data %>% 
    specify(formula = color ~ NULL, success = "red") %>% 
    generate(reps = 1000, type = "bootstrap") %>% 
    calculate(stat = "prop")
}

# Compute nested data frame with sampled data, sample proportions, all 
# bootstrap replicates, and percentile_ci
balls_percentile_cis <- bowl %>% 
  rep_sample_n(size = 50, reps = 100, replace = FALSE) %>% 
  group_by(replicate) %>% 
  nest() %>% 
  mutate(sample_prop = map_dbl(data, ~mean(.x$color == "red"))) %>%
  # run infer pipeline on each nested tibble to generated bootstrap replicates
  mutate(bootstraps = map(data, bootstrap_pipeline)) %>% 
  group_by(replicate) %>% 
  # Compute 95% percentile CI's for each nested element
  mutate(percentile_ci = map(bootstraps, get_ci, type = "percentile", level = 0.80))

# Identify if confidence interval captured true p
percentile_cis <- balls_percentile_cis %>% 
  unnest(percentile_ci) %>% 
  mutate(captured = `10%` <= p_red & p_red <= `90%`)
    
# Plot them!
ggplot(percentile_cis) +
  geom_segment(aes(
    y = replicate, yend = replicate, x = `10%`, xend = `90%`, 
    color = factor(captured, levels = c("TRUE", "FALSE"))
  )) +
  scale_color_manual(values=c("lightgrey","black"), labels=c("Yes", "No")) +
  labs(x = expression("Proportion of red balls"), 
       y = "Confidence interval number", 
       color = "Captured") +
  geom_vline(xintercept = p_red, color = "red") + 
  coord_cartesian(xlim = c(0.1, 0.7)) + 
  theme_bw(16) + 
  theme(panel.grid.major.y = element_blank(), 
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank()
        )
```


1. Based on this plot, what condfidence level do you think was used to construct these confidence intervals? What do you base this answer on?

2. Why is the statement "The 95% confidence interval ranges between 2.76 and 5.31, so there is a 95% probability the true population mean is between 2.76 and 5.31" incorrect?


## Chapter 9 Problems

Problems one through four apply to the resume and gender bias hypothesis testing example in [ModernDive Chapter 9.1 - 9.2](https://moderndive.netlify.app/9-hypothesis-testing.html#ht-activity)

1. What was the point of repeatedly permuting (i.e., "shuffling") the gender label across all the promotion decisions?

2. The difference in the proportion of promoted males and the proportoin of promoted females was calculated from each randomly permuted data set. What did this collection of difference scores allow us to construct?

3. What was the null hypothesis in this example?

4. Why did we reject the null hypothesis?


## Chapter 10 Problems

1. According to the bivariate regression model described in [ModernDive Section 10.1.1](https://moderndive.com/10-inference-for-regression.html#teaching-evaluations-analysis), what is the predicted difference in teaching score between a professor with a beauty score of 4 and a beauty score 0f 5? What value in the regression table tells us this value and why?

2. Imagine that the relationship of teaching evaluation scores and percieved beauty was studied at every large university across the country, using surveys from 463 courses at each university just as the original UT Austin study did. Because of sampling variability, we would expect that the slope of the `score ~ bty_avg` bivariate regression model fit to each data set would be different. Based on the regression model fit to the original UT Austin data and described in [ModernDive Section 10.1.1](https://moderndive.com/10-inference-for-regression.html#teaching-evaluations-analysis), what would we expect the standard deviation of all these different slopes to be?

3. According to the regression model described in [ModernDive Section 10.1.1](https://moderndive.com/10-inference-for-regression.html#teaching-evaluations-analysis), what is the probability of observing a slope greater than or equal to .067, assuming the "null" distribution of slopes is the true sampling distribution of slopes?

4. Explain [Figure 10.11](https://moderndive.com/10-inference-for-regression.html#fig:p-value-slope) in about one paragraph, written as though you were talking to another student in the course. 